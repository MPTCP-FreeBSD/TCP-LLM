{"cells":[{"cell_type":"markdown","metadata":{"id":"NRrQXL0DlaZw"},"source":["The folliwing are state, actions and reward for TCP_LLm project"]},{"cell_type":"markdown","metadata":{"id":"033UK0NYlaZy"},"source":["Feature Encoder\n","Rewards"]},{"cell_type":"code","execution_count":25,"metadata":{"id":"s5SWNkEZlaZz","outputId":"61133f69-7f15-4217-92f8-b57593f54411"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","\n","class EncoderNetwork(nn.Module):\n","    \"\"\"\n","    Customized state encoder for TCP tasks.\n","    \"\"\"\n","    def __init__(self, conv_size=4, embed_dim=128):\n","        super().__init__()\n","        self.past_k = conv_size\n","        self.embed_dim = embed_dim\n","        self.fc1 = nn.Sequential(nn.Linear(1, embed_dim), nn.LeakyReLU())  # current RTT\n","        self.fc2 = nn.Sequential(nn.Linear(1, embed_dim), nn.LeakyReLU())  # current loss rate\n","        self.conv3 = nn.Sequential(nn.Conv1d(1, embed_dim, conv_size), nn.LeakyReLU(), nn.Flatten())  # past k throughputs\n","        # No congestion window layer\n","\n","    def forward(self, state):\n","        # Update this based on the new state shape and TCP features\n","        batch_size, seq_len = state.shape[0], state.shape[1]\n","        state = state.reshape(batch_size * seq_len, -1)  # Assuming new state shape for TCP\n","\n","        current_rtt = state[..., 0:1]  # RTT\n","        current_loss = state[..., 1:2]  # Loss rate\n","        throughputs = state[..., 2:3]  # Past k throughputs\n","\n","        features1 = self.fc1(current_rtt).reshape(batch_size, seq_len, -1)\n","        features2 = self.fc2(current_loss).reshape(batch_size, seq_len, -1)\n","        features3 = self.conv3(throughputs).reshape(batch_size, seq_len, -1)\n","\n","        return features1, features2, features3  # Return the processed TCP features\n"]},{"cell_type":"markdown","metadata":{"id":"HdKm4kSBlaZ0"},"source":["Rewards"]},{"cell_type":"code","execution_count":26,"metadata":{"id":"V8lo1ncdlaZ1"},"outputs":[],"source":["import numpy as np\n","import torch\n","\n","def compute_tcp_reward(features, reward_weights):\n","    \"\"\"\n","    Compute the reward for TCP flow control based on features extracted by the encoder.\n","\n","    Args:\n","    - features (tuple of tensors): Extracted features from the encoder, including RTT, loss rate, and throughputs.\n","    - reward_weights (dict): Weights for each feature type to scale the reward.\n","\n","    Returns:\n","    - float: The computed reward value.\n","    \"\"\"\n","    # Extract features\n","    current_rtt = features[0].mean(dim=1)  # Average across the sequence length\n","    current_loss = features[1].mean(dim=1)\n","    throughputs = features[2].mean(dim=1)\n","\n","    # Convert to numpy for reward computation\n","    current_rtt = current_rtt.cpu().detach().numpy()\n","    current_loss = current_loss.cpu().detach().numpy()\n","    throughputs = throughputs.cpu().detach().numpy()\n","\n","    # Ensure that the values are positive to avoid log(0) or negative values\n","    throughput = np.clip(throughputs, 1e-6, None)\n","    latency = np.clip(current_rtt, 1e-6, None)\n","    loss_rate = np.clip(current_loss, 1e-6, None)\n","\n","    # Compute the reward\n","    reward = reward_weights['throughput'] * np.log(throughput) - \\\n","             reward_weights['latency'] * np.log(latency) - \\\n","             reward_weights['loss_rate'] * np.log(loss_rate)\n","\n","    return reward.mean()  # Return the mean reward for the batch\n","\n","# Example usage:\n","reward_weights = {'throughput': 0.5, 'latency': 0.25, 'loss_rate': 0.25}\n"]},{"cell_type":"markdown","metadata":{"id":"bmIB8pdPlaZ1"},"source":["RL Policy"]},{"cell_type":"code","execution_count":27,"metadata":{"id":"2zPqghRGmhpa"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","\n","class EncoderNetwork(nn.Module):\n","    \"\"\"\n","    Customized state encoder for TCP tasks.\n","    \"\"\"\n","    def __init__(self, conv_size=4, embed_dim=128):\n","        super().__init__()\n","        self.past_k = conv_size\n","        self.embed_dim = embed_dim\n","        self.fc1 = nn.Sequential(nn.Linear(1, embed_dim), nn.LeakyReLU())  # current RTT\n","        self.fc2 = nn.Sequential(nn.Linear(1, embed_dim), nn.LeakyReLU())  # current loss rate\n","        self.conv3 = nn.Sequential(nn.Conv1d(1, embed_dim, conv_size), nn.LeakyReLU(), nn.Flatten())  # past k throughputs\n","        # No congestion window layer\n","\n","    def forward(self, state):\n","        # Update this based on the new state shape and TCP features\n","        batch_size, seq_len = state.shape[0], state.shape[1]\n","        state = state.reshape(batch_size * seq_len, -1)  # Assuming new state shape for TCP\n","\n","        current_rtt = state[..., 0:1]  # RTT\n","        current_loss = state[..., 1:2]  # Loss rate\n","        throughputs = state[..., 2:3]  # Past k throughputs\n","\n","        features1 = self.fc1(current_rtt).reshape(batch_size, seq_len, -1)\n","        features2 = self.fc2(current_loss).reshape(batch_size, seq_len, -1)\n","        features3 = self.conv3(throughputs).reshape(batch_size, seq_len, -1)\n","\n","        return features1, features2, features3  # Return the processed TCP features\n","\n","\n","\n","\n","import numpy as np\n","import torch\n","\n","def compute_tcp_reward(features, reward_weights):\n","    \"\"\"\n","    Compute the reward for TCP flow control based on features extracted by the encoder.\n","\n","    Args:\n","    - features (tuple of tensors): Extracted features from the encoder, including RTT, loss rate, and throughputs.\n","    - reward_weights (dict): Weights for each feature type to scale the reward.\n","\n","    Returns:\n","    - float: The computed reward value.\n","    \"\"\"\n","    # Extract features\n","    current_rtt = features[0].mean(dim=1)  # Average across the sequence length\n","    current_loss = features[1].mean(dim=1)\n","    throughputs = features[2].mean(dim=1)\n","\n","    # Convert to numpy for reward computation\n","    current_rtt = current_rtt.cpu().detach().numpy()\n","    current_loss = current_loss.cpu().detach().numpy()\n","    throughputs = throughputs.cpu().detach().numpy()\n","\n","    # Ensure that the values are positive to avoid log(0) or negative values\n","    throughput = np.clip(throughputs, 1e-6, None)\n","    latency = np.clip(current_rtt, 1e-6, None)\n","    loss_rate = np.clip(current_loss, 1e-6, None)\n","\n","    # Compute the reward\n","    reward = reward_weights['throughput'] * np.log(throughput) - \\\n","             reward_weights['latency'] * np.log(latency) - \\\n","             reward_weights['loss_rate'] * np.log(loss_rate)\n","\n","    return reward.mean()  # Return the mean reward for the batch\n","\n","# Example usage:\n","reward_weights = {'throughput': 0.5, 'latency': 0.25, 'loss_rate': 0.25}\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","import torch\n","import torch.nn as nn\n","from collections import deque\n","\n","class OfflineRLPolicy(nn.Module):\n","    def __init__(\n","            self,\n","            state_feature_dim,\n","            action_dim,  # Changed from bitrate_levels to action_dim\n","            state_encoder,\n","            plm,\n","            plm_embed_size,\n","            max_length=None,\n","            max_ep_len=100,\n","            device='cuda' if torch.cuda.is_available() else 'cpu',\n","            device_out=None,\n","            residual=False,\n","            conv_size=4,\n","            which_layer=-1,  # for early stopping: specify which layer to stop\n","            **kwargs\n","    ):\n","        super().__init__()\n","\n","        if device_out is None:\n","            device_out = device\n","\n","        self.action_dim = action_dim  # Changed from bitrate_levels to action_dim\n","        self.max_length = max_length\n","\n","        self.plm = plm\n","        self.plm_embed_size = plm_embed_size\n","\n","        # =========== multimodal encoder (start) ===========\n","        self.state_encoder = state_encoder\n","        self.state_feature_dim = state_feature_dim\n","        self.embed_timestep = nn.Embedding(max_ep_len + 1, plm_embed_size).to(device)\n","        self.embed_return = nn.Linear(1, plm_embed_size).to(device)\n","        self.embed_action = nn.Linear(1, plm_embed_size).to(device)\n","        self.embed_state1 = nn.Linear(state_feature_dim, plm_embed_size).to(device)  # RTT\n","        self.embed_state2 = nn.Linear(state_feature_dim, plm_embed_size).to(device)  # Loss rate\n","        self.embed_state3 = nn.Linear(state_feature_dim * (6 - conv_size + 1), plm_embed_size).to(device)  # Throughput\n","\n","        self.embed_ln = nn.LayerNorm(plm_embed_size).to(device)\n","        # =========== multimodal encoder (end) ===========\n","\n","        self.action_head = nn.Linear(plm_embed_size, action_dim).to(device)  # Changed to action_dim\n","\n","        self.device = device\n","        self.device_out = device_out\n","\n","        # the following are used for evaluation\n","        self.states_dq = deque([torch.zeros((1, 0, plm_embed_size), device=device)], maxlen=max_length)\n","        self.returns_dq = deque([torch.zeros((1, 0, plm_embed_size), device=device)], maxlen=max_length)\n","        self.actions_dq = deque([torch.zeros((1, 0, plm_embed_size), device=device)], maxlen=max_length)\n","\n","        self.residual = residual\n","        self.which_layer = which_layer\n","        self.modules_except_plm = nn.ModuleList([  # used to save and load modules except plm\n","            self.state_encoder, self.embed_timestep, self.embed_return, self.embed_action, self.embed_ln,\n","            self.embed_state1, self.embed_state2, self.embed_state3, self.action_head\n","        ])\n","\n","    def forward(self, states, actions, returns, timesteps, attention_mask=None):\n","        \"\"\"\n","        Forward function, used for training.\n","        \"\"\"\n","        assert actions.shape[0] == 1, 'batch size should be 1 to avoid CUDA memory exceed'\n","\n","        # Step 1: process actions, returns, and timesteps first as they are simple\n","        actions = actions.to(self.device)  # shape: (1, seq_len, 1)\n","        returns = returns.to(self.device)  # shape: (1, seq_len, 1)\n","        timesteps = timesteps.to(self.device)  # shape: (1, seq_len)\n","\n","        # 1.1 embed action, return, timestep\n","        action_embeddings = self.embed_action(actions)  # shape: (1, seq_len, embed_size)\n","        returns_embeddings = self.embed_return(returns)  # shape: (1, seq_len, embed_size)\n","        time_embeddings = self.embed_timestep(timesteps)  # shape: (1, seq_len, embed_size)\n","\n","        # 1.2 time embeddings are treated similar to positional embeddings\n","        action_embeddings = action_embeddings + time_embeddings\n","        returns_embeddings = returns_embeddings + time_embeddings\n","\n","        # Step 2: process states, turn them into embeddings.\n","        states = states.to(self.device)  # shape: (1, seq_len, features)\n","        features1, features2, features3 = self.state_encoder(states)\n","\n","        # Embed each feature output\n","        states_embeddings1 = self.embed_state1(features1) + time_embeddings  # RTT\n","        states_embeddings2 = self.embed_state2(features2) + time_embeddings  # Loss rate\n","        states_embeddings3 = self.embed_state3(features3) + time_embeddings  # Throughput\n","\n","        # Step 3: stack returns, states, actions embeddings.\n","        stacked_inputs = []\n","        action_embed_positions = np.zeros(returns_embeddings.shape[1])  # record the positions of action embeddings\n","        for i in range(returns_embeddings.shape[1]):\n","            stacked_input = torch.cat((returns_embeddings[0, i:i + 1], states_embeddings1[0, i:i + 1],\n","                                       states_embeddings2[0, i:i + 1], states_embeddings3[0, i:i + 1],\n","                                       action_embeddings[0, i:i + 1]), dim=0)\n","            stacked_inputs.append(stacked_input)\n","            action_embed_positions[i] = (i + 1) * (2 + 3)  # Updated for 3 features\n","        stacked_inputs = torch.cat(stacked_inputs, dim=0).unsqueeze(0)\n","        stacked_inputs = stacked_inputs[:, -self.plm_embed_size:, :]  # truncate sequence length\n","        stacked_inputs_ln = self.embed_ln(stacked_inputs)  # layer normalization\n","\n","        # Step 4: feed stacked embeddings into the plm\n","        if attention_mask is None:\n","            attention_mask = torch.ones((stacked_inputs_ln.shape[0], stacked_inputs_ln.shape[1]), dtype=torch.long, device=self.device)\n","\n","        transformer_outputs = self.plm(\n","            inputs_embeds=stacked_inputs_ln,\n","            attention_mask=attention_mask,\n","            output_hidden_states=True,\n","            stop_layer_idx=self.which_layer,\n","        )\n","        logits = transformer_outputs['last_hidden_state']\n","        if self.residual:\n","            logits = logits + stacked_inputs_ln  # residual add\n","\n","        # Step 5: predict actions\n","        logits_used = logits[:, action_embed_positions - 2]\n","        action_pred = self.action_head(logits_used)\n","\n","        return action_pred\n"]},{"cell_type":"code","execution_count":28,"metadata":{"id":"sNBDA9YclaZ2","outputId":"c1c8b4bd-369d-4005-cef8-37acaa8d1bfa"},"outputs":[],"source":["# import pandas as pd\n","\n","# # Specify the paths to your six CSV files\n","# file1 = '/Users/shyamshrestha/Desktop/my data/client1_pcc.csv'\n","# file2 = '/Users/shyamshrestha/Desktop/untitled folder/ client1 _bbr_full.csv'\n","# file3 = '/Users/shyamshrestha/Desktop/untitled folder/client2_pcc.csv'\n","# file4 = '/Users/shyamshrestha/Desktop/untitled folder/client1_cubic_full.csv'\n","# file5 = '/Users/shyamshrestha/Desktop/untitled folder/client2_bbr.csv'\n","# file6 = '/Users/shyamshrestha/Desktop/untitled folder/client2_cubic.csv'\n","\n","# # List of file paths\n","# file_paths = [file1, file2, file3, file4, file5, file6]\n","\n","# # List to hold data from all files\n","# data_frames = []\n","\n","# # Read each CSV file and append to the list\n","# for file in file_paths:\n","#     df = pd.read_csv(file)\n","#     data_frames.append(df)\n","\n","# # Concatenate all data frames into one\n","# combined_df = pd.concat(data_frames, ignore_index=True)\n","\n","# # Specify the output file name\n","# output_file = '/Users/shyamshrestha/Desktop/untitled folder/allcombined.csv'\n","\n","# # Save the combined data frame to a new CSV file\n","# combined_df.to_csv(output_file, index=False)\n","\n","# print(f\"All CSV files combined into {output_file}\")\n"]},{"cell_type":"markdown","metadata":{"id":"THjZxtppmDV6"},"source":["experience pool"]},{"cell_type":"code","execution_count":30,"metadata":{"id":"UtOe_0gzmGmZ"},"outputs":[{"ename":"FileNotFoundError","evalue":"[Errno 2] No such file or directory: '/content/encoded_file.csv'","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[30], line 64\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;66;03m# Load your data\u001b[39;00m\n\u001b[1;32m     63\u001b[0m data_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/content/encoded_file.csv\u001b[39m\u001b[38;5;124m'\u001b[39m  \u001b[38;5;66;03m# Replace with your actual data file path\u001b[39;00m\n\u001b[0;32m---> 64\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;66;03m# Create an instance of ExperiencePool\u001b[39;00m\n\u001b[1;32m     67\u001b[0m exp_pool \u001b[38;5;241m=\u001b[39m ExperiencePool()\n","File \u001b[0;32m~/anaconda3/envs/urllc/lib/python3.8/site-packages/pandas/io/parsers/readers.py:912\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m    899\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    900\u001b[0m     dialect,\n\u001b[1;32m    901\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    908\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m    909\u001b[0m )\n\u001b[1;32m    910\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 912\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m~/anaconda3/envs/urllc/lib/python3.8/site-packages/pandas/io/parsers/readers.py:577\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    574\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    576\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 577\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    579\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    580\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n","File \u001b[0;32m~/anaconda3/envs/urllc/lib/python3.8/site-packages/pandas/io/parsers/readers.py:1407\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1404\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1406\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1407\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m~/anaconda3/envs/urllc/lib/python3.8/site-packages/pandas/io/parsers/readers.py:1661\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1659\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1660\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1661\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1662\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1663\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1664\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1665\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1666\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1667\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1668\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1669\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1670\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1671\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1672\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n","File \u001b[0;32m~/anaconda3/envs/urllc/lib/python3.8/site-packages/pandas/io/common.py:859\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    854\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    855\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    856\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    857\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    858\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 859\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    860\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    861\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    862\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    863\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    864\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    865\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    866\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    867\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    868\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/encoded_file.csv'"]}],"source":["import numpy as np\n","import pandas as pd\n","import pickle\n","import torch\n","\n","def compute_tcp_reward(features, reward_weights):\n","    \"\"\"\n","    Compute the reward for TCP flow control based on features extracted by the encoder.\n","\n","    Args:\n","    - features (tuple of tensors): Extracted features from the encoder, including RTT, loss rate, and throughputs.\n","    - reward_weights (dict): Weights for each feature type to scale the reward.\n","\n","    Returns:\n","    - float: The computed reward value.\n","    \"\"\"\n","    # Extract features\n","    current_rtt = features[0]\n","    current_loss = features[1]\n","    throughputs = features[2]\n","\n","    # Convert to numpy for reward computation\n","    current_rtt = current_rtt.cpu().detach().numpy()\n","    current_loss = current_loss.cpu().detach().numpy()\n","    throughputs = throughputs.cpu().detach().numpy()\n","\n","    # Ensure that the values are positive to avoid log(0) or negative values\n","    throughput = np.clip(throughputs, 1e-6, None)\n","    latency = np.clip(current_rtt, 1e-6, None)\n","    loss_rate = np.clip(current_loss, 1e-6, None)\n","\n","    # Compute the reward\n","    reward = reward_weights['throughput'] * np.log(throughput) - \\\n","             reward_weights['latency'] * np.log(latency) - \\\n","             reward_weights['loss_rate'] * np.log(loss_rate)\n","\n","    return reward.mean()  # Return the mean reward for the batch\n","\n","class ExperiencePool:\n","    \"\"\"\n","    Experience pool for collecting trajectories.\n","    \"\"\"\n","    def __init__(self):\n","        self.states = []\n","        self.actions = []\n","        self.rewards = []\n","        self.dones = []\n","\n","    def add(self, state, action, reward, done):\n","        self.states.append(state)  # sometimes state is also called obs (observation)\n","        self.actions.append(action)\n","        self.rewards.append(reward)\n","        self.dones.append(done)\n","\n","    def __len__(self):\n","        return len(self.states)\n","\n","\n","# Define the CCA mapping\n","cca_mapping = {'Cubic': 0, 'BBR': 1, 'PCC': 2}\n","\n","# Load your data\n","data_path = '/content/encoded_file.csv'  # Replace with your actual data file path\n","df = pd.read_csv(data_path)\n","\n","# Create an instance of ExperiencePool\n","exp_pool = ExperiencePool()\n","\n","# Initialize the global reward variable\n","global_reward = 0\n","\n","# Iterate through each row and update the experience pool\n","for index, row in df.iterrows():\n","    # Extract state features\n","    current_rtt = row['Latency']  # RTT\n","    current_loss = row['LossRate']  # Loss rate\n","    throughput = row['Throughput']  # Throughput\n","\n","    # Normalize or preprocess if needed\n","    state = np.array([current_rtt, current_loss, throughput])\n","\n","    # Reshape state to match the expected input of EncoderNetwork\n","    # Assuming seq_len = 1 for a single time step\n","    state_tensor = torch.tensor(state, dtype=torch.float).unsqueeze(0).unsqueeze(0)  # Shape (1, 1, 3)\n","\n","    # Action is CCA selection\n","    action = row['CCAs']  # Already in numeric format\n","\n","    # Compute reward based on your reward function\n","    reward_weights = {'throughput': 0.5, 'latency': 0.25, 'loss_rate': 0.25}\n","    reward = compute_tcp_reward(\n","        features=(state_tensor[:, :, 0:1], state_tensor[:, :, 1:2], state_tensor[:, :, 2:3]),\n","        reward_weights=reward_weights\n","    )\n","\n","    # Assuming 'done' is an indicator of end of an episode, set it accordingly\n","    done = 0  # Placeholder, adjust according to your logic\n","\n","    # Add to experience pool\n","    exp_pool.add(state=state, action=action, reward=reward, done=done)\n","\n","# Save the experience pool\n","pickle_save_path = 'exp_pool.pkl'\n","pickle.dump(exp_pool, open(pickle_save_path, 'wb'))\n","print(f\"Done. Experience pool saved at:\", pickle_save_path)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_ezAWT-EmIcG"},"outputs":[],"source":["import os\n","import sys\n","import numpy as np\n","import torch\n","import pickle\n","\n","from argparse import ArgumentParser, ArgumentDefaultsHelpFormatter\n","from pprint import pprint\n","from munch import Munch\n","from torch.nn import CrossEntropyLoss\n","from torch.optim import AdamW\n","from torch.optim.lr_scheduler import LambdaLR\n","from torch.utils.tensorboard import SummaryWriter\n","from baseline_special.utils.utils import load_traces\n","from plm_special.trainer import Trainer\n","from plm_special.evaluate import evaluate_on_env\n","from plm_special.test import test_on_env\n","from plm_special.data.dataset import ExperienceDataset\n","from plm_special.models.rl_policy import OfflineRLPolicy\n","from plm_special.models.state_encoder import EncoderNetwork\n","from plm_special.utils.utils import set_random_seed\n","from plm_special.utils.plm_utils import load_plm\n","from plm_special.utils.console_logger import ConsoleLogger\n","from config import cfg\n","\n","\n","PLM_LAYER_SIZES = {\n","    'gpt2': {\n","        'base': 24,\n","        'small': 12,\n","        'large': 36,\n","        'xl': 48\n","    },\n","    'llama': {\n","        'base': 32,\n","    },\n","    't5-lm': {\n","        'base': 12,\n","        'small': 6,\n","        'large': 24,\n","        'xl': 24\n","    }\n","}\n","\n","# Updated reward computation function\n","def process_reward(reward,\n","                   max_reward=exp_dataset_info.max_reward,\n","                   min_reward=exp_dataset_info.min_reward,\n","                   scale=args.scale):\n","    # Assuming reward is already scaled in your implementation\n","    return reward\n","\n","def save_model(args, model, save_dir):\n","    if args.rank > 0:\n","        model.plm.save_pretrained(save_dir)\n","        torch.save(model.modules_except_plm.state_dict(), os.path.join(save_dir, 'modules_except_plm.bin'))\n","    else:\n","        torch.save(model.state_dict(), os.path.join(save_dir, 'model.bin'))\n","\n","def load_model(args, model, model_dir):\n","    if args.rank > 0:\n","        model.plm.load_adapter(model_dir, adapter_name='default')\n","        model.modules_except_plm.load_state_dict(torch.load(os.path.join(model_dir, 'modules_except_plm.bin')))\n","    else:\n","        model.load_state_dict(torch.load(os.path.join(model_dir, 'model.bin')))\n","    return model\n","\n","def adapt(args, model, exp_dataset, exp_dataset_info, eval_env_settings, checkpoint_dir, best_model_dir, eval_process_reward_fn):\n","    optimizer = AdamW(\n","        model.parameters(),\n","        lr=args.lr,\n","        weight_decay=args.weight_decay,\n","    )\n","    lr_scheduler = LambdaLR(\n","        optimizer,\n","        lambda steps: min((steps + 1) / args.warmup_steps, 1)\n","    )\n","    loss_fn = CrossEntropyLoss()\n","    trainer = Trainer(args, model=model, optimizer=optimizer, exp_dataset=exp_dataset, loss_fn=loss_fn, device=args.device, lr_scheduler=lr_scheduler,\n","                      grad_accum_steps=args.grad_accum_steps)\n","\n","    target_return = exp_dataset_info.max_return * args.target_return_scale\n","    best_eval_return = 0.\n","\n","    total_train_losses = []\n","    for epoch in range(args.num_epochs):\n","        train_logs, train_losses = trainer.train_epoch()\n","        total_train_losses.extend(train_losses)\n","        print('='* 20, f'Training Iteration #{epoch}', '=' * 20)\n","        print('>' * 10, 'Training Information:')\n","        pprint(train_logs)\n","\n","        if epoch % args.save_checkpoint_per_epoch == 0:\n","            checkpoint_dir_epoch = os.path.join(checkpoint_dir, str(epoch))\n","            if not os.path.exists(checkpoint_dir_epoch):\n","                os.makedirs(checkpoint_dir_epoch)\n","            save_model(args, model, checkpoint_dir_epoch)\n","            print('Checkpoint saved at:', checkpoint_dir_epoch)\n","\n","        if epoch % args.eval_per_epoch == 0:\n","            eval_logs = evaluate_on_env(args, env_settings=eval_env_settings, model=model, target_return=target_return, max_ep_num=args.trace_num,\n","                                        process_reward_fn=eval_process_reward_fn)\n","            episodes_return = eval_logs['episodes_return']\n","            if best_eval_return < episodes_return:\n","                best_eval_return = episodes_return\n","                save_model(args, model, best_model_dir)\n","                print('Best model saved at:', best_model_dir)\n","\n","            eval_logs['best_return'] = best_eval_return\n","            print('>' * 10, 'Evaluation Information')\n","            pprint(eval_logs)\n","    train_losses_path = os.path.join(checkpoint_dir, 'train_losses.txt')\n","    np.savetxt(train_losses_path, total_train_losses, fmt='%.6f', delimiter='\\n')\n","\n","\n","def test(args, model, exp_dataset_info, env_settings, model_dir, result_dir, test_process_reward_fn):\n","    model = load_model(args, model, model_dir)\n","    print('Load model from:', model_dir)\n","    target_return = exp_dataset_info.max_return * args.target_return_scale\n","    results = test_on_env(args, model, result_dir, env_settings, target_return, args.trace_num, test_process_reward_fn, seed=args.seed)\n","    print(results)\n","    print('Test time:', results['time'], '\\nMean reward:', results['mean_reward'])\n","    print('Results saved at:', result_dir)\n","\n","\n","def run(args):\n","    assert args.plm_type in cfg.plm_types\n","    assert args.plm_size in cfg.plm_sizes\n","    assert args.exp_pool_path is not None, 'please specify a experience pool path for training'\n","    assert args.trace in cfg.trace_dirs.keys()\n","    assert args.video in cfg.video_size_dirs.keys()\n","\n","    set_random_seed(args.seed)\n","\n","    trace_dir = cfg.trace_dirs[args.trace]\n","    video_size_dir = cfg.video_size_dirs[args.video]\n","    all_cooked_time ,all_cooked_bw ,all_file_names, all_mahimahi_ptrs = load_traces(trace_dir)\n","    args.trace_num = min(args.trace_num, len(all_file_names))\n","    if args.trace_num == -1:\n","        args.trace_num = len(all_file_names)\n","    if args.trace_num == len(all_file_names):\n","        args.fixed_order = True\n","\n","    env_settings = {\n","        'all_cooked_time': all_cooked_time,\n","        'all_cooked_bw': all_cooked_bw,\n","        'all_file_names': all_file_names,\n","        'all_mahimahi_ptrs': all_mahimahi_ptrs,\n","        'video_size_dir': video_size_dir,\n","        'fixed': args.fixed_order,\n","        'trace_num': args.trace_num,\n","    }\n","\n","    exp_pool = pickle.load(open(args.exp_pool_path, 'rb'))\n","    exp_dataset = ExperienceDataset(exp_pool, gamma=args.gamma, scale=args.scale, max_length=args.w, sample_step=args.sample_step)\n","    exp_dataset_info = Munch(exp_dataset.exp_dataset_info)\n","    print('Experience dataset info:')\n","    pprint(exp_dataset_info)\n","\n","    plm, *_ = load_plm(args.plm_type, os.path.join(cfg.plm_dir, args.plm_type, args.plm_size),\n","                       device_input_side=args.device, device_output_side=args.device_out, device_middle_side=args.device_mid)\n","\n","    if args.plm_type != 'llama':\n","        plm = plm.to(args.device)\n","\n","    if args.rank != -1:\n","        plm = peft_model(plm, args.plm_type, rank=args.rank)\n","\n","    assert args.state_feature_dim is not None, 'please specify state feature dim to create state encoder'\n","    state_encoder = EncoderNetwork(embed_dim=args.state_feature_dim)\n","    state_encoder = state_encoder.to(args.device)\n","\n","    plm_embed_size = cfg.plm_embed_sizes[args.plm_type][args.plm_size]\n","    max_ep_len = exp_dataset_info.max_timestep + 1\n","    rl_policy = OfflineRLPolicy(state_feature_dim=args.state_feature_dim, bitrate_levels=BITRATE_LEVELS, state_encoder=state_encoder, plm=plm, plm_embed_size=plm_embed_size,\n","                                           max_length=args.w, max_ep_len=max_ep_len, device=args.device, device_out=args.device_out, which_layer=args.which_layer)\n","\n","    train_exp_pool_info = args.exp_pool_path.split('/')[-4:-1]\n","    train_exp_pool_info = '_'.join(train_exp_pool_info)\n","    models_dir = os.path.join(cfg.plm_ft_dir, f'{args.plm_type}_{args.plm_size}', train_exp_pool_info + f'_ss_{args.sample_step}', f'rank_{args.rank}_w_{args.w}_gamma_{args.gamma}_sfd_{args.state_feature_dim}'\\\n","                              f'_lr_{args.lr}_wd_{args.weight_decay}_warm_{args.warmup_steps}_epochs_{args.num_epochs}_seed_{args.seed}')\n","    results_dir = os.path.join(cfg.results_dir, f'{args.trace}_{args.video}', f'trace_num_{args.trace_num}_fixed_{args.fixed_order}', f'{args.plm_type}_{args.plm_size}',\n","                               f'early_stop_{args.which_layer}_rank_{args.rank}_w_{args.w}_gamma_{args.gamma}_sfd_{args.state_feature_dim}_target_return_{args.target_return_scale}')\n","\n","    print('Models saved at:', models_dir)\n","    print('Results saved at:', results_dir)\n","\n","    if not os.path.exists(models_dir):\n","        os.makedirs(models_dir)\n","    if not os.path.exists(results_dir):\n","        os.makedirs(results_dir)\n","\n","    # Selective adaption or testing\n","    if args.mode == 'train':\n","        adapt(args, rl_policy, exp_dataset, exp_dataset_info, env_settings, models_dir, os.path.join(models_dir, 'best'), eval_process_reward_fn=process_reward)\n","    elif args.mode == 'test':\n","        test(args, rl_policy, exp_dataset_info, env_settings, models_dir, results_dir, test_process_reward_fn=process_reward)\n","    else:\n","        raise NotImplementedError(f'Unsupported mode: {args.mode}')\n","\n","\n","def main():\n","    parser = ArgumentParser(formatter_class=ArgumentDefaultsHelpFormatter)\n","    parser.add_argument('--exp_pool_path', type=str, default=None, help='experience pool path')\n","    parser.add_argument('--num_epochs', type=int, default=50, help='number of epochs')\n","    parser.add_argument('--lr', type=float, default=1e-4, help='learning rate')\n","    parser.add_argument('--weight_decay', type=float, default=1e-4, help='weight decay')\n","    parser.add_argument('--warmup_steps', type=int, default=1000, help='warmup steps for learning rate scheduler')\n","    parser.add_argument('--grad_accum_steps', type=int, default=1, help='gradient accumulation steps')\n","    parser.add_argument('--rank', type=int, default=-1, help='Rank of PEFT model')\n","    parser.add_argument('--plm_type', type=str, default='gpt2', help='pre-trained language model type')\n","    parser.add_argument('--plm_size', type=str, default='small', help='pre-trained language model size')\n","    parser.add_argument('--w', type=int, default=40, help='temporal window size')\n","    parser.add_argument('--sample_step', type=int, default=1, help='sample step')\n","    parser.add_argument('--gamma', type=float, default=0.99, help='discount factor')\n","    parser.add_argument('--state_feature_dim', type=int, default=None, help='state feature dimension')\n","    parser.add_argument('--target_return_scale', type=float, default=1.0, help='target return scale')\n","    parser.add_argument('--which_layer', type=int, default=-1, help='which layer to extract hidden states as input of policy')\n","    parser.add_argument('--trace_num', type=int, default=-1, help='number of traces')\n","    parser.add_argument('--trace', type=str, default='high', help='trace')\n","    parser.add_argument('--video', type=str, default='high', help='video size')\n","    parser.add_argument('--fixed_order', type=bool, default=False, help='whether to load trace in fixed order')\n","    parser.add_argument('--scale', type=bool, default=False, help='whether to scale the reward')\n","\n","    parser.add_argument('--save_checkpoint_per_epoch', type=int, default=10, help='save checkpoint per number of epochs')\n","    parser.add_argument('--eval_per_epoch', type=int, default=5, help='evaluation per number of epochs')\n","    parser.add_argument('--mode', type=str, default='train', help='train or test')\n","\n","    # Device arguments\n","    parser.add_argument('--device', type=str, default='cuda', help='Device for state_encoder and plm')\n","    parser.add_argument('--device_mid', type=str, default='cuda', help='Device for middle layers of plm')\n","    parser.add_argument('--device_out', type=str, default='cuda', help='Device for output layers of plm')\n","\n","    parser.add_argument('--seed', type=int, default=42, help='random seed')\n","\n","    args = parser.parse_args()\n","    run(args)\n","\n","\n","if __name__ == '__main__':\n","    main()\n","\n"]},{"cell_type":"code","execution_count":44,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Done. Experience pool saved at: exp_pool_with_cca_actions.pkl\n"]}],"source":["import pandas as pd\n","import numpy as np\n","import pickle\n","\n","class ExperiencePool:\n","    \"\"\"\n","    Experience pool for collecting trajectories.\n","    \"\"\"\n","    def __init__(self):\n","        self.states = []\n","        self.actions = []\n","        self.rewards = []\n","        self.dones = []\n"," \n","    def add(self, state, action, reward, done):\n","        self.states.append(state)  # sometimes state is also called observation (obs)\n","        self.actions.append(action)\n","        self.rewards.append(reward)\n","        self.dones.append(done)\n"," \n","    def __len__(self):\n","        return len(self.states)\n","\n","# Load your dataset from a CSV file\n","csv_file_path = '/home/sit-research/Desktop/NetLLM-master/tcp-llm_dataset/encoded_file2.csv'  # Replace with the actual path to your dataset\n","df = pd.read_csv(csv_file_path)\n","\n","# Define the list of columns to include in the state\n","columns_to_use = [\n","    'Throughput',     # Measured throughput\n","    'LossRate',       # Packet loss rate\n","    'Latency',        # Round-trip time (RTT)\n","    'SendingRate',    # Sending rate of the flow\n","]\n","for column in columns_to_use:\n","    df[column] = df[column].astype(np.float32)\n","# Initialize the experience pool\n","exp_pool = ExperiencePool()\n","\n","# Iterate through each row and calculate rewards\n","for index, row in df.iterrows():\n","    # Create state from relevant columns\n","\n","\n","    state = np.array(row[columns_to_use])\n","    \n","    # Example reward function: Maximize throughput, minimize latency and packet loss\n","    throughput = row['Throughput']\n","    latency = row['Latency']\n","    packet_loss = row['LossRate']\n","    \n","    # Reward function\n","    reward = throughput / (latency + 1) - packet_loss\n","    \n","    # Use the current CCA as the action\n","    cca_action = row['CCAs']  # Assumes CCAs are already encoded as integers\n","    \n","    # Add to experience pool\n","    exp_pool.add(state=state, action=np.float32(cca_action), reward=np.float32(reward), done=0)\n","    break\n","\n","# Save the experience pool using pickle\n","pickle_save_path = 'exp_pool_with_cca_actions.pkl'\n","pickle.dump(exp_pool, open(pickle_save_path, 'wb'))\n","print(f\"Done. Experience pool saved at: {pickle_save_path}\")\n"]},{"cell_type":"code","execution_count":42,"metadata":{},"outputs":[{"data":{"text/plain":["dtype('O')"]},"execution_count":42,"metadata":{},"output_type":"execute_result"}],"source":["exp_pool.states[0].dtype"]},{"cell_type":"code","execution_count":33,"metadata":{},"outputs":[{"data":{"text/plain":["list"]},"execution_count":33,"metadata":{},"output_type":"execute_result"}],"source":["type(exp_pool.states)"]},{"cell_type":"code","execution_count":34,"metadata":{},"outputs":[{"data":{"text/plain":["1"]},"execution_count":34,"metadata":{},"output_type":"execute_result"}],"source":["len(exp_pool.states)"]},{"cell_type":"code","execution_count":35,"metadata":{},"outputs":[{"data":{"text/plain":["9069003.0"]},"execution_count":35,"metadata":{},"output_type":"execute_result"}],"source":["exp_pool.states[0][0]"]},{"cell_type":"code","execution_count":36,"metadata":{},"outputs":[{"data":{"text/plain":["0.0034533278085291386"]},"execution_count":36,"metadata":{},"output_type":"execute_result"}],"source":["exp_pool.states[0][1]"]},{"cell_type":"code","execution_count":37,"metadata":{},"outputs":[{"data":{"text/plain":["0.0002651966060511768"]},"execution_count":37,"metadata":{},"output_type":"execute_result"}],"source":["exp_pool.states[0][2]"]},{"cell_type":"code","execution_count":38,"metadata":{},"outputs":[{"data":{"text/plain":["441188.5625"]},"execution_count":38,"metadata":{},"output_type":"execute_result"}],"source":["exp_pool.states[0][3]"]},{"cell_type":"code","execution_count":39,"metadata":{},"outputs":[{"data":{"text/plain":["numpy.ndarray"]},"execution_count":39,"metadata":{},"output_type":"execute_result"}],"source":["type(exp_pool.states[0])"]},{"cell_type":"code","execution_count":40,"metadata":{},"outputs":[{"data":{"text/plain":["numpy.float32"]},"execution_count":40,"metadata":{},"output_type":"execute_result"}],"source":["type(exp_pool.actions[0])"]},{"cell_type":"code","execution_count":41,"metadata":{},"outputs":[{"data":{"text/plain":["list"]},"execution_count":41,"metadata":{},"output_type":"execute_result"}],"source":["type(exp_pool.actions)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","import pickle\n","import torch\n","\n","def compute_tcp_reward(features, reward_weights):\n","    \"\"\"\n","    Compute the reward for TCP flow control based on features extracted by the encoder.\n","\n","    Args:\n","    - features (tuple of tensors): Extracted features from the encoder, including RTT, loss rate, and throughputs.\n","    - reward_weights (dict): Weights for each feature type to scale the reward.\n","\n","    Returns:\n","    - np.array: The computed reward value.\n","    \"\"\"\n","    # Extract features\n","    current_rtt = features[0]\n","    current_loss = features[1]\n","    throughputs = features[2]\n","\n","    # Convert to numpy for reward computation\n","    current_rtt = current_rtt.cpu().detach().numpy()\n","    current_loss = current_loss.cpu().detach().numpy()\n","    throughputs = throughputs.cpu().detach().numpy()\n","\n","    # Ensure that the values are positive to avoid log(0) or negative values\n","    throughput = np.clip(throughputs, 1e-6, None)\n","    latency = np.clip(current_rtt, 1e-6, None)\n","    loss_rate = np.clip(current_loss, 1e-6, None)\n","\n","    # Compute the reward\n","    reward = reward_weights['throughput'] * np.log(throughput) - \\\n","             reward_weights['latency'] * np.log(latency) - \\\n","             reward_weights['loss_rate'] * np.log(loss_rate)\n","\n","    return reward.mean()  # Return the mean reward for the batch\n","\n","class ExperiencePool:\n","    \"\"\"\n","    Experience pool for collecting trajectories.\n","    \"\"\"\n","    def __init__(self):\n","        self.states = []\n","        self.actions = []\n","        self.rewards = []\n","        self.dones = []\n","\n","    def add(self, state, action, reward, done):\n","        self.states.append(np.array(state))  # Store state as np.array\n","        self.actions.append(np.array(action))  # Store action as np.array\n","        self.rewards.append(np.array(reward))  # Store reward as np.array\n","        self.dones.append(np.array(done))  # Store done as np.array\n","\n","    def __len__(self):\n","        return len(self.states)\n","\n","# Define the CCA mapping\n","cca_mapping = {0: 'Cubic', 1: 'BBR', 2: 'PCC'}  # Adjusted mapping for actions\n","\n","# Load your data\n","data_path = '/home/sit-research/Desktop/NetLLM-master/tcp-llm_dataset/encoded_file.csv'  # Replace with your actual data file path\n","df = pd.read_csv(data_path)\n","\n","# Create an instance of ExperiencePool\n","exp_pool = ExperiencePool()\n","\n","# Initialize the global reward variable\n","global_reward = 0\n","\n","# Iterate through each row and update the experience pool\n","for index, row in df.iterrows():\n","    # Extract state features\n","    current_rtt = row['Latency']  # RTT\n","    current_loss = row['LossRate']  # Loss rate\n","    throughput = row['Throughput']  # Throughput\n","\n","    # Normalize or preprocess if needed\n","    state = np.array([current_rtt, current_loss, throughput])\n","\n","    # Reshape state to match the expected input of EncoderNetwork\n","    # Assuming seq_len = 1 for a single time step\n","    state_tensor = torch.tensor(state, dtype=torch.float).unsqueeze(0).unsqueeze(0)  # Shape (1, 1, 3)\n","\n","    # Action is CCA selection\n","    action = np.array(row['CCAs'])  # Convert action to np.array\n","\n","    # Compute reward based on your reward function\n","    reward_weights = {'throughput': 0.5, 'latency': 0.25, 'loss_rate': 0.25}\n","    reward = compute_tcp_reward(\n","        features=(state_tensor[:, :, 0:1], state_tensor[:, :, 1:2], state_tensor[:, :, 2:3]),\n","        reward_weights=reward_weights\n","    )\n","\n","    # Convert reward to np.array\n","    reward = np.array(reward)\n","\n","    # Assuming 'done' is an indicator of end of an episode, set it accordingly\n","    done = np.array(0)  # Placeholder, adjust according to your logic\n","\n","    # Add to experience pool\n","    exp_pool.add(state=state, action=action, reward=reward, done=done)\n","\n","# Save the experience pool\n","pickle_save_path = 'exp_pool_tllm.pkl'\n","with open(pickle_save_path, 'wb') as f:\n","    pickle.dump(exp_pool, f)\n","\n","print(f\"Done. Experience pool saved at:\", pickle_save_path)\n"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["python: can't open file 'train_script.py': [Errno 2] No such file or directory\n"]}],"source":["!python train_script.py --which-layer 3\n"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.19"}},"nbformat":4,"nbformat_minor":0}
