{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import torch\n",
    "\n",
    "def compute_tcp_reward(features, reward_weights):\n",
    "    \"\"\"\n",
    "    Compute the reward for TCP flow control based on features extracted by the encoder.\n",
    "\n",
    "    Args:\n",
    "    - features (tuple of tensors): Extracted features from the encoder, including RTT, loss rate, and throughputs.\n",
    "    - reward_weights (dict): Weights for each feature type to scale the reward.\n",
    "\n",
    "    Returns:\n",
    "    - np.array: The computed reward value.\n",
    "    \"\"\"\n",
    "    # Extract features\n",
    "    current_rtt = features[0]\n",
    "    current_loss = features[1]\n",
    "    throughputs = features[2]\n",
    "\n",
    "    # Convert to numpy for reward computation\n",
    "    current_rtt = current_rtt.cpu().detach().numpy()\n",
    "    current_loss = current_loss.cpu().detach().numpy()\n",
    "    throughputs = throughputs.cpu().detach().numpy()\n",
    "\n",
    "    # Ensure that the values are positive to avoid log(0) or negative values\n",
    "    throughput = np.clip(throughputs, 1e-6, None)\n",
    "    latency = np.clip(current_rtt, 1e-6, None)\n",
    "    loss_rate = np.clip(current_loss, 1e-6, None)\n",
    "\n",
    "    # Compute the reward\n",
    "    reward = reward_weights['throughput'] * np.log(throughput) - \\\n",
    "             reward_weights['latency'] * np.log(latency) - \\\n",
    "             reward_weights['loss_rate'] * np.log(loss_rate)\n",
    "\n",
    "    return reward.mean()  # Return the mean reward for the batch\n",
    "\n",
    "class ExperiencePool:\n",
    "    \"\"\"\n",
    "    Experience pool for collecting trajectories.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.states = []\n",
    "        self.actions = []\n",
    "        self.rewards = []\n",
    "        self.dones = []\n",
    "\n",
    "    def add(self, state, action, reward, done):\n",
    "        self.states.append(np.array(state))  # Store state as np.array\n",
    "        self.actions.append(np.array(action))  # Store action as np.array\n",
    "        self.rewards.append(np.array(reward))  # Store reward as np.array\n",
    "        self.dones.append(np.array(done))  # Store done as np.array\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.states)\n",
    "\n",
    "# Define the CCA mapping\n",
    "cca_mapping = {'Cubic': 0, 'BBR': 1, 'PCC': 2}\n",
    "\n",
    "# Load your data\n",
    "data_path = '/content/encoded_file.csv'  # Replace with your actual data file path\n",
    "df = pd.read_csv(data_path)\n",
    "\n",
    "# Create an instance of ExperiencePool\n",
    "exp_pool = ExperiencePool()\n",
    "\n",
    "# Initialize the global reward variable\n",
    "global_reward = 0\n",
    "\n",
    "# Iterate through each row and update the experience pool\n",
    "for index, row in df.iterrows():\n",
    "    # Extract state features\n",
    "    current_rtt = row['Latency']  # RTT\n",
    "    current_loss = row['LossRate']  # Loss rate\n",
    "    throughput = row['Throughput']  # Throughput\n",
    "\n",
    "    # Normalize or preprocess if needed\n",
    "    state = np.array([current_rtt, current_loss, throughput])\n",
    "\n",
    "    # Reshape state to match the expected input of EncoderNetwork\n",
    "    # Assuming seq_len = 1 for a single time step\n",
    "    state_tensor = torch.tensor(state, dtype=torch.float).unsqueeze(0).unsqueeze(0)  # Shape (1, 1, 3)\n",
    "\n",
    "    # Action is CCA selection\n",
    "    action = np.array(row['CCAs'])  # Convert action to np.array\n",
    "\n",
    "    # Compute reward based on your reward function\n",
    "    reward_weights = {'throughput': 0.5, 'latency': 0.25, 'loss_rate': 0.25}\n",
    "    reward = compute_tcp_reward(\n",
    "        features=(state_tensor[:, :, 0:1], state_tensor[:, :, 1:2], state_tensor[:, :, 2:3]),\n",
    "        reward_weights=reward_weights\n",
    "    )\n",
    "\n",
    "    # Convert reward to np.array\n",
    "    reward = np.array(reward)\n",
    "\n",
    "    # Assuming 'done' is an indicator of end of an episode, set it accordingly\n",
    "    done = np.array(0)  # Placeholder, adjust according to your logic\n",
    "\n",
    "    # Add to experience pool\n",
    "    exp_pool.add(state=state, action=action, reward=reward, done=done)\n",
    "\n",
    "# Save the experience pool\n",
    "pickle_save_path = 'exp_pool.pkl'\n",
    "pickle.dump(exp_pool, open(pickle_save_path, 'wb'))\n",
    "print(f\"Done. Experience pool saved at:\", pickle_save_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class EncoderNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    The encoder network for encoding each piece of information of the state.\n",
    "    This design is adapted from Pensieve/Genet for customized data.\n",
    "    \"\"\"\n",
    "    def __init__(self, conv_size=4, embed_dim=128):\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "\n",
    "        # Fully connected layers for specific features\n",
    "        self.fc_rtt = nn.Sequential(\n",
    "            nn.Linear(1, embed_dim),  # RTT\n",
    "            nn.LeakyReLU()\n",
    "        )\n",
    "        self.fc_loss = nn.Sequential(\n",
    "            nn.Linear(1, embed_dim),  # Loss rateC\n",
    "            nn.LeakyReLU()\n",
    "        )\n",
    "        \n",
    "        # Convolutional layers for sequence data\n",
    "        self.conv_throughput = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=1, out_channels=embed_dim, kernel_size=conv_size),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.MaxPool1d(kernel_size=2),\n",
    "            nn.Flatten()\n",
    "        )  # Convolutional layer for throughput\n",
    "        \n",
    "        # Fully connected layer to combine features\n",
    "        self.fc_combined = nn.Sequential(\n",
    "            nn.Linear(embed_dim * 2 + embed_dim, embed_dim),  # Adjust input size to combined features\n",
    "            nn.LeakyReLU()\n",
    "        )\n",
    "\n",
    "    def forward(self, state):\n",
    "        \"\"\"\n",
    "        Forward pass of the encoder network.\n",
    "        \n",
    "        Args:\n",
    "        - state (torch.Tensor): Input tensor of shape (batch_size, seq_len, num_features).\n",
    "        \n",
    "        Returns:\n",
    "        - torch.Tensor: Encoded features of shape (batch_size, seq_len, embed_dim).\n",
    "        \"\"\"\n",
    "        batch_size, seq_len = state.shape[0], state.shape[1]\n",
    "\n",
    "        # Separate features\n",
    "        rtt = state[:, :, 0:1].transpose(1, 2)  # Shape: (batch_size, 1, seq_len)\n",
    "        loss = state[:, :, 1:2].transpose(1, 2)  # Shape: (batch_size, 1, seq_len)\n",
    "        throughput = state[:, :, 2:3].transpose(1, 2)  # Shape: (batch_size, 1, seq_len)\n",
    "        \n",
    "        # Apply convolutions and fully connected layers\n",
    "        features_rtt = self.fc_rtt(rtt).reshape(batch_size, seq_len, -1)\n",
    "        features_loss = self.fc_loss(loss).reshape(batch_size, seq_len, -1)\n",
    "        features_throughput = self.conv_throughput(throughput).reshape(batch_size, seq_len, -1)\n",
    "        \n",
    "        # Concatenate features\n",
    "        combined_features = torch.cat([features_rtt, features_loss, features_throughput], dim=-1)\n",
    "        \n",
    "        # Pass through fully connected layer\n",
    "        final_features = self.fc_combined(combined_features)\n",
    "        \n",
    "        return final_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mOfflineRLPolicy\u001b[39;00m(\u001b[43mnn\u001b[49m\u001b[38;5;241m.\u001b[39mModule):\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, state_feature_dim, num_actions):\n\u001b[1;32m      3\u001b[0m         \u001b[38;5;28msuper\u001b[39m(OfflineRLPolicy, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'nn' is not defined"
     ]
    }
   ],
   "source": [
    "class OfflineRLPolicy(nn.Module):\n",
    "    def __init__(self, state_feature_dim, num_actions):\n",
    "        super(OfflineRLPolicy, self).__init__()\n",
    "        self.state_encoder = EncoderNetwork(input_dim_rtt=1, input_dim_loss_rate=1, input_dim_throughput=1)\n",
    "        self.embed_state1 = nn.Linear(state_feature_dim, 64)\n",
    "        self.embed_state2 = nn.Linear(state_feature_dim, 64)\n",
    "        self.embed_time = nn.Linear(state_feature_dim, 64)\n",
    "        self.fc_policy = nn.Linear(64 * 3, num_actions)\n",
    "\n",
    "    def forward(self, states):\n",
    "        # Encode states using EncoderNetwork\n",
    "        encoded_states = self.state_encoder(states)\n",
    "        # Embedding states\n",
    "        state_embedding1 = F.relu(self.embed_state1(encoded_states))\n",
    "        state_embedding2 = F.relu(self.embed_state2(encoded_states))\n",
    "        # Assume time embeddings are handled elsewhere\n",
    "        combined_features = torch.cat([state_embedding1, state_embedding2], dim=-1)\n",
    "        policy_output = self.fc_policy(combined_features)\n",
    "        return policy_output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "need to check the following ex pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done. Experience pool saved at: exp_pool_tllm.pkl\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import torch\n",
    "\n",
    "def compute_tcp_reward(features, reward_weights):\n",
    "    \"\"\"\n",
    "    Compute the reward for TCP flow control based on features extracted by the encoder.\n",
    "\n",
    "    Args:\n",
    "    - features (tuple of tensors): Extracted features from the encoder, including RTT, loss rate, and throughputs.\n",
    "    - reward_weights (dict): Weights for each feature type to scale the reward.\n",
    "\n",
    "    Returns:\n",
    "    - np.array: The computed reward value.\n",
    "    \"\"\"\n",
    "    # Extract features\n",
    "    current_rtt = features[0]\n",
    "    current_loss = features[1]\n",
    "    throughputs = features[2]\n",
    "\n",
    "    # Convert to numpy for reward computation\n",
    "    current_rtt = current_rtt.cpu().detach().numpy()\n",
    "    current_loss = current_loss.cpu().detach().numpy()\n",
    "    throughputs = throughputs.cpu().detach().numpy()\n",
    "\n",
    "    # Ensure that the values are positive to avoid log(0) or negative values\n",
    "    throughput = np.clip(throughputs, 1e-6, None)\n",
    "    latency = np.clip(current_rtt, 1e-6, None)\n",
    "    loss_rate = np.clip(current_loss, 1e-6, None)\n",
    "\n",
    "    # Compute the reward\n",
    "    reward = reward_weights['throughput'] * np.log(throughput) - \\\n",
    "             reward_weights['latency'] * np.log(latency) - \\\n",
    "             reward_weights['loss_rate'] * np.log(loss_rate)\n",
    "\n",
    "    return reward.mean()  # Return the mean reward for the batch\n",
    "\n",
    "class ExperiencePool:\n",
    "    \"\"\"\n",
    "    Experience pool for collecting trajectories.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.states = []\n",
    "        self.actions = []\n",
    "        self.rewards = []\n",
    "        self.dones = []\n",
    "\n",
    "    def add(self, state, action, reward, done):\n",
    "        self.states.append(np.array(state))  # Store state as np.array\n",
    "        self.actions.append(np.array(action))  # Store action as np.array\n",
    "        self.rewards.append(np.array(reward))  # Store reward as np.array\n",
    "        self.dones.append(np.array(done))  # Store done as np.array\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.states)\n",
    "\n",
    "# Define the CCA mapping\n",
    "cca_mapping = {0: 'Cubic', 1: 'BBR', 2: 'PCC'}  # Adjusted mapping for actions\n",
    "\n",
    "# Load your data\n",
    "data_path = '/home/sit-research/Desktop/NetLLM-master/tcp-llm_dataset/encoded_file.csv'  # Replace with your actual data file path\n",
    "df = pd.read_csv(data_path)\n",
    "\n",
    "# Create an instance of ExperiencePool\n",
    "exp_pool = ExperiencePool()\n",
    "\n",
    "# Initialize the global reward variable\n",
    "global_reward = 0\n",
    "\n",
    "# Iterate through each row and update the experience pool\n",
    "for index, row in df.iterrows():\n",
    "    # Extract state features\n",
    "    current_rtt = row['Latency']  # RTT\n",
    "    current_loss = row['LossRate']  # Loss rate\n",
    "    throughput = row['Throughput']  # Throughput\n",
    "\n",
    "    # Normalize or preprocess if needed\n",
    "    state = np.array([current_rtt, current_loss, throughput])\n",
    "\n",
    "    # Reshape state to match the expected input of EncoderNetwork\n",
    "    # Assuming seq_len = 1 for a single time step\n",
    "    state_tensor = torch.tensor(state, dtype=torch.float).unsqueeze(0).unsqueeze(0)  # Shape (1, 1, 3)\n",
    "\n",
    "    # Action is CCA selection\n",
    "    action = np.array(row['CCAs'])  # Convert action to np.array\n",
    "\n",
    "    # Compute reward based on your reward function\n",
    "    reward_weights = {'throughput': 0.5, 'latency': 0.25, 'loss_rate': 0.25}\n",
    "    reward = compute_tcp_reward(\n",
    "        features=(state_tensor[:, :, 0:1], state_tensor[:, :, 1:2], state_tensor[:, :, 2:3]),\n",
    "        reward_weights=reward_weights\n",
    "    )\n",
    "\n",
    "    # Convert reward to np.array\n",
    "    reward = np.array(reward)\n",
    "\n",
    "    # Assuming 'done' is an indicator of end of an episode, set it accordingly\n",
    "    done = np.array(0)  # Placeholder, adjust according to your logic\n",
    "\n",
    "    # Add to experience pool\n",
    "    exp_pool.add(state=state, action=action, reward=reward, done=done)\n",
    "\n",
    "# Save the experience pool\n",
    "pickle_save_path = 'exp_pool_tllm.pkl'\n",
    "with open(pickle_save_path, 'wb') as f:\n",
    "    pickle.dump(exp_pool, f)\n",
    "\n",
    "print(f\"Done. Experience pool saved at:\", pickle_save_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "New ex pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "urllc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
