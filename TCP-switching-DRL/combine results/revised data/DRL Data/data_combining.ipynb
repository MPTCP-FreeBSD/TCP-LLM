{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(             Timestamp  Throughput  LossRate   Latency    SendingRate\n",
       " 0  2024-05-30 05:59:18     8605643  0.003613  0.000277  462352.077293\n",
       " 1  2024-05-30 05:59:19    10015816  0.000334  0.000262  482968.052886\n",
       " 2  2024-05-30 05:59:20     7806168  0.000195  0.000277  473153.500192\n",
       " 3  2024-05-30 05:59:21     6004856  0.000148  0.000376  563905.927247\n",
       " 4  2024-05-30 05:59:22     3835752  0.000127  0.000349  546822.613613,\n",
       " (634, 5))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import pandas as pd\n",
    "# import os\n",
    "\n",
    "# # List the files\n",
    "# file_names = [\n",
    "#     '/Users/shyamshrestha/Downloads/combine results/revised data/DRL Data/ client1 _bbr_full.csv',\n",
    "#     '/Users/shyamshrestha/Downloads/combine results/revised data/DRL Data/client1_cubic_full.csv',\n",
    "#     '/Users/shyamshrestha/Downloads/combine results/revised data/DRL Data/client1_pcc.csv',\n",
    "#     '/Users/shyamshrestha/Downloads/combine results/revised data/DRL Data/client2_bbr_switch_cubic.csv',\n",
    "#     '/Users/shyamshrestha/Downloads/combine results/revised data/DRL Data/client2_pcc_switch_bbr.csv',\n",
    "#     '/Users/shyamshrestha/Downloads/combine results/revised data/DRL Data/client2_pcc.csv'\n",
    "# ]\n",
    "\n",
    "# # Load the data from the files into pandas DataFrames\n",
    "# file_paths = [os.path.join(\"/mnt/data\", file_name) for file_name in file_names]\n",
    "# data_frames = []\n",
    "\n",
    "# for file_path in file_paths:\n",
    "#     try:\n",
    "#         df = pd.read_csv(file_path)\n",
    "#         data_frames.append(df)\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error loading {file_path}: {e}\")\n",
    "\n",
    "# # Combine all data frames into one\n",
    "# combined_df = pd.concat(data_frames, ignore_index=True)\n",
    "\n",
    "# # Save the combined DataFrame as a CSV file in the same directory\n",
    "# combined_csv_path = os.path.join(os.path.dirname(file_paths[0]), 'combined_data.csv')\n",
    "# combined_df.to_csv(combined_csv_path, index=False)\n",
    "\n",
    "# combined_df.head(), combined_df.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Timestamp  Throughput  LossRate   Latency    SendingRate\n",
      "0  1717048758     8605643  0.003613  0.000277  462352.077293\n",
      "1  1717048759    10015816  0.000334  0.000262  482968.052886\n",
      "2  1717048760     7806168  0.000195  0.000277  473153.500192\n",
      "3  1717048761     6004856  0.000148  0.000376  563905.927247\n",
      "4  1717048762     3835752  0.000127  0.000349  546822.613613\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load data\n",
    "data = pd.read_csv('/Users/shyamshrestha/Downloads/combine results/revised data/DRL Data/combined_data.csv')\n",
    "\n",
    "# Convert timestamp to Unix timestamp\n",
    "data['Timestamp'] = pd.to_datetime(data['Timestamp'])\n",
    "data['Timestamp'] = (data['Timestamp'] - pd.Timestamp(\"1970-01-01\")) // pd.Timedelta('1s')\n",
    "\n",
    "# Now your 'Timestamp' column contains Unix timestamps\n",
    "print(data.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Check for missing values\n",
    "missing_values = data.isnull().sum()\n",
    "print(\"Missing values:\")\n",
    "print(missing_values)\n",
    "\n",
    "# Imputation\n",
    "# Replace missing values with the mean of the column\n",
    "data.fillna(data.mean(), inplace=True)\n",
    "\n",
    "# After handling missing values, you can proceed with your analysis\n",
    "print(data.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "\n",
    "# Preprocess data\n",
    "data['Timestamp'] = pd.to_datetime(data['Timestamp'])  # Convert timestamp column to datetime type\n",
    "data = data.dropna()  # Drop rows with missing values, adjust as needed\n",
    "\n",
    "# Normalize numerical columns\n",
    "scaler = MinMaxScaler()\n",
    "data[['Throughput', 'LossRate', 'Latency', 'SendingRate']] = scaler.fit_transform(data[['Throughput', 'LossRate', 'Latency', 'SendingRate']])\n",
    "\n",
    "# Save preprocessed data to a new CSV file\n",
    "data.to_csv('preprocessed_data.csv', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
